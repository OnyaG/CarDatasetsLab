# ПСИАПД
# Отчет по 1 лабораторной

Подготовили студенты группы 449м
- Ташкова Анна
- Старков Силантий

## Содержание
---
1. [Теоретическая база](#теоретическая-база)
2. [Описание разработанной системы (алгоритмы, принципы работы, архитектура)](#[описание-разработанной-системы])
3. [Результаты работы и тестирования системы (скриншоты, изображения, графики, закономерности)](#[результаты-работы-и-тестирования-системы])
4. [Выводы по работе](#[выводы-по-работе])
5. [Использованные источники](#[использованные-источники])
---
## Теоретическая база
### Сверточные нейронные сети
Сверточные нейронные сети (Convolutional Neural Networks, CNN) – класс алгоритмов машинного обучения. С их помощью удается достичь впечатляющих результатов в области распознавания образов, классификации изображений, а также обработки и анализа видеоданных.
В состав сверточной нейронной сети входит несколько слоев. От количества слоев зависит мощность архитектуры и эффективность обучения. 
#### Схема основных составляющих свёрточной нейронной сети:
- сверточный слой;
- пулинг;
- нормализация по батчу;
- полносвязный слой.
---
В процессе **свертки** нейронная сеть удаляет ненужное и оставляет полезное, т.е. то, что нужно для анализа изображения. В качестве примера можно привести линии, края и плоские области. **Свертка** может быть создана **для каждого признака**. Нейронная сеть сама подбирает их, когда выполняет распознавание и классификацию в каждом слое свертки.

Следующим после слоя свертки идет **слой пулинга**. В нем **из признаков**, отобранных **слоем свертки**, выбираются наиболее **важные** и удаляются несущественные. **Слой свертки** может быть снова применен к результатам, полученным в процессе пулинга, и **повторен несколько раз**. Это необходимо для построения иерархии признаков, начиная с самых **примитивных** (фрагменты контуров) и заканчивая **сложными** (кошачьи глаза или форма ушей).

**Задача первых слоев** сверточной нейронной сети – **анализ мельчайших элементов изображения** (ворсинки, трещинки и т.д.), размер которого может составлять 2 x 2 или 3 x 3 пикселя. Такой маленький формат не позволяет определить глаза или уши кошки или деревья, на которых она сидит. Однако можно найти различия в цвете и свете – грани между различными объектами. **Для следующих слоев характерны сложные объекты**, такие как круги и другие фигуры.

Пакетный слой (**батч**) (англ. batch gradient descent) — реализация **градиентного спуска**, когда на каждой итерации обучающая выборка просматривается целиком, и только после этого изменяются веса модели.

В **полносвязных слоях** каждый нейрон соединён со всеми нейронами предыдущего слоя. Полносвязные слои обычно используются **в конце сети** для **преобразования признаков**, извлечённых предыдущими слоями, в **окончательные результаты**. Например, в задаче классификации изображений полносвязные слои могут преобразовывать признаки объекта (формы и текстуры) в выводы о принадлежности изображения к тому или иному классу объектов.

---
### ResNet
**ResNet** — это сокращенное название для **Residual Network** (дословно  — «остаточная сеть»).
**При увеличении "плоских" слоев** в стиле VGG **качество сети падает**, а не растет (даже на обучающей выборке, не говоря уже о тестовой). **При большой глубине НС возникает проблема** затухающего градиента, таким образом, сеть перестает обучаться. 
Чтобы **преодолеть эту проблему**, Microsoft ввела **глубокую «остаточную» структуру обучения**. Вместо того, чтобы надеяться на то, что каждые несколько stacked layers непосредственно соответствуют желаемому основному представлению, они явно позволяют этим слоям соответствовать «остаточному». **Формулировка F(x) + x** может быть реализована с помощью **нейронных сетей с соединениями для быстрого доступа**.
![resnet-570x328](https://github.com/user-attachments/assets/92eac38b-0343-4e37-8dbe-60d8bc9dc721)

Рисунок 1

**Соединения быстрого доступа (shortcut connections)** пропускают один или несколько слоев и выполняют сопоставление идентификаторов. Их выходы добавляются к выходам stacked layers. Используя ResNet, можно решить множество проблем, таких как:
- ResNet относительно легко оптимизировать: «простые» сети (которые просто складывают слои) показывают большую ошибку обучения, когда глубина увеличивается.
- ResNet позволяет относительно легко увеличить точность благодаря увеличению глубины, чего с другими сетями добиться сложнее.
Таким образом мы дополнительно прокидываем тождественные связи (identity) между входом и выходом остаточной функции (боремся с затухающим градиентом). Эти тождественные связи носят название "skip-connections" или "short-cuts".

### Архитектурные особенности  
![res_block3](https://github.com/user-attachments/assets/c56a826c-b140-4f1d-bfe5-1e5dfaa1f90d)

Рисунок 2

- Перед последним полносвязным слоем применяется GAP (Global Average Pooling)
- Принцип сохранения сложности (уменьшаем пространственный размер в 2 раза одновременно увеличиваем в 2 раза кол-во карт признаков)
- Почти везде уменьшение размерности через свертки с шагом (stride) = 2
- BatchNorm используется после каждой свертки

### Архитектура ResNet
![resnet-architecture-3](https://github.com/user-attachments/assets/5fb92df0-bd7f-4cde-bb6a-526040fa8256)

Рисунок 3

Простая сеть: Простые базовые линии (рис. 3, в центре) в основном вдохновлены философией сетей VGG (рис. 3, слева). Сверточные слои в основном имеют фильтры 3×3 и следуют двум простым правилам:

1. Для одной и той же выходной карты объектов слои имеют одинаковое количество фильтров;
2. Если размер карты объектов уменьшается вдвое, число фильтров удваивается, чтобы сохранить временную сложность каждого слоя.
Стоит отметить, что модель ResNet имеет меньше фильтров и сложность меньше, чем сети VGG.

ResNet: на основе описанной выше простой сети добавлено быстрое соединение (рис. 3, справа), которое превращает сеть в ее остаточную версию. Идентификационные быстрые соединения F (x {W} + x) могут использоваться непосредственно, когда вход и выход имеют одинаковые размерности (быстрые соединения сплошной линии на рис. 3). Когда размерности увеличиваются (пунктирные линии на рис. 3), он рассматривает два варианта:

1. Быстрое соединение выполняет сопоставление идентификаторов с дополнительными нулями, добавленными для увеличения размерности. Эта опция не вводит никаких дополнительных параметров.
2. Проекция быстрого соединения  в F (x {W} + x) используется для сопоставления размерностей (выполнено с помощью 1×1 сверток).

Каждый блок ResNet имеет два уровня глубины (используется в небольших сетях, таких как ResNet 18, 34) или 3 уровня (ResNet 50, 101, 152).

**50-слойная ResNet**: каждый 3-слойный блок заменяется в 34-слойной сети этим 3-слойным узким местом, в результате получается 50-слойная ResNet (см. Таблицу выше). Они используют вариант 2 для увеличения размерностей. Эта модель имеет 3,8 миллиарда FLOPs.


### Оптимизатор **AdaMax**

AdaMax - оптимизатор на основе Adam (— adaptive moment estimation, оптимизационный алгоритм. Он сочетает в себе и идею накопления движения и идею более слабого обновления весов для типичных признаков.)

![image](https://github.com/user-attachments/assets/120b3712-1560-4579-8d02-bb643e94ce03)


![image](https://github.com/user-attachments/assets/06cfb312-cebe-4811-8be7-cf54d15e5e3f)



## Описание разработанной системы
---
## Результаты работы и тестирования системы
---
## Выводы по работе
В ходе выполнения лабораторной работы, в разделе теоретической базы были рассмотрены **сверточные нейронные сети**, рассмотрена архитектура **ResNet** и оптимизатор **AdaMax**.

---
## Использованные источники
- [Сверточные нейронные сети](https://gb.ru/blog/svertochnye-nejronnye-seti)
- [Типы слоёв нейронной сети](https://aisec.cs.msu.ru/section_robust_ml/nn_architectures/)
- [Архитектуры нейронных сетей](https://aisec.cs.msu.ru/section_robust_ml/nn_architectures/)
- [ResNet](https://neurohive.io/ru/vidy-nejrosetej/resnet-34-50-101/)
- [AdaMax](https://habr.com/ru/articles/318970/)
